# K-Nearest Neighbors (kNN)

**Type**:  Unsupervised | Regression | Classification

**Summary**:  The basic idea is to find the ‘k’ closest data points in the training set to a given test data point and use the labels of those closest points to make a prediction for the test point.

## Table of contents
- [When to use](#when-to-use)
- [Intuition](#intuition)
- [Formal definition / math](#formal-definition--math)
- [Algorithm (step-by-step)](#algorithm-step-by-step)
- [Complexity](#complexity)
- [Hyperparameters](#hyperparameters)
- [Implementation (python)](#implementation-python)
- [Practical tips & gotchas](#practical-tips--gotchas)
- [Experiments / example](#experiments--example)
- [References & further reading](#references--further-reading)
- [Exercises / extension ideas](#exercises--extension-ideas)
- [Changelog](#changelog)

---

## When to use
- Short bullet list of use-cases and unsuitable scenarios.

## Intuition
- Plain-language explanation and mental model.
- Visual idea if helpful (e.g., how decision boundaries look, how gradients flow).

## Formal definition / math
- Core equations.
- Loss functions, constraints, or objective.
- Key theorems or guarantees (if any).

## Algorithm (step-by-step)
1. Step 1 — ...
2. Step 2 — ...
3. Step 3 — ...
- Pseudocode block:

```text
Algorithm <Name>
Input: ...
Output: ...
for i in 1..N:
    ...
return ...